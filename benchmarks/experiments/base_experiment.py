"""
Base Experiment Framework for Refiner Benchmarking
==================================================

æä¾› Refiner ç®—æ³•è¯„æµ‹çš„åŸºç¡€ç±»å’Œæ•°æ®æ¨¡å‹ã€‚

è®¾è®¡åŸåˆ™ï¼š
1. ä¸ sage.middleware.operators.rag.evaluate ä¸­çš„è¯„æµ‹æŒ‡æ ‡é›†æˆ
2. æ”¯æŒå¤šç§ Refiner ç®—æ³•çš„ç»Ÿä¸€è¯„æµ‹
3. æ”¯æŒé…ç½®é©±åŠ¨çš„å®éªŒç®¡ç†
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any

import yaml


class RefinerAlgorithm(str, Enum):
    """æ”¯æŒçš„ Refiner ç®—æ³•"""

    BASELINE = "baseline"  # æ— å‹ç¼©åŸºçº¿
    LONGREFINER = "longrefiner"  # LongRefiner ä¸‰é˜¶æ®µå‹ç¼©
    REFORM = "reform"  # REFORM æ³¨æ„åŠ›å¤´å‹ç¼©
    PROVENCE = "provence"  # Provence å¥å­çº§å‰ªæ
    SIMPLE = "simple"  # ç®€å•æˆªæ–­
    RECOMP = "recomp"  # RECOMP (TODO)
    LLMLINGUA2 = "llmlingua2"  # LLMLingua-2: BERT token classification compression
    LONGLLMLINGUA = "longllmlingua"  # LongLLMLingua: Question-aware long context compression

    @classmethod
    def available(cls) -> list[str]:
        """è¿”å›å·²å®ç°çš„ç®—æ³•åˆ—è¡¨"""
        return [
            cls.BASELINE.value,
            cls.LONGREFINER.value,
            cls.REFORM.value,
            cls.PROVENCE.value,
            cls.LLMLINGUA2.value,
            cls.LONGLLMLINGUA.value,
        ]


class DatasetType(str, Enum):
    """æ”¯æŒçš„æ•°æ®é›†ç±»å‹"""

    NQ = "nq"  # Natural Questions
    HOTPOTQA = "hotpotqa"
    TRIVIAQA = "triviaqa"
    SQUAD = "squad"
    ASQA = "asqa"
    CUSTOM = "custom"


# FlashRAG æ”¯æŒçš„æ•°æ®é›†åˆ—è¡¨
AVAILABLE_DATASETS = [
    "nq",  # Natural Questions
    "triviaqa",  # TriviaQA
    "hotpotqa",  # HotpotQA (multi-hop)
    "2wikimultihopqa",  # 2Wiki Multi-hop
    "musique",  # Musique (multi-hop)
    "asqa",  # ASQA (long-form)
    "popqa",  # PopQA
    "webq",  # WebQuestions
]


@dataclass
class RefinerExperimentConfig:
    """Refiner è¯„æµ‹å®éªŒé…ç½®"""

    # === åŸºç¡€é…ç½® ===
    name: str = "refiner_experiment"
    description: str = ""

    # === æ•°æ®é…ç½® ===
    dataset: DatasetType = DatasetType.NQ  # å‘åå…¼å®¹ï¼Œå•æ•°æ®é›†
    datasets: list[str] = field(default_factory=lambda: ["nq"])  # å¤šæ•°æ®é›†æ”¯æŒ
    dataset_config: str = "nq"  # HuggingFace dataset config
    split: str = "test"
    max_samples: int = 100

    # === ç®—æ³•é…ç½® ===
    algorithms: list[str] = field(default_factory=lambda: ["baseline", "longrefiner"])
    budget: int = 2048  # Token é¢„ç®—

    # === æ£€ç´¢é…ç½® ===
    retriever_type: str = "wiki18_faiss"
    top_k: int = 100
    index_path: str = ""
    documents_path: str = ""

    # === ç”Ÿæˆé…ç½® ===
    generator_model: str = "Llama-3.1-8B-Instruct"
    generator_base_url: str = "http://localhost:8889/v1"

    # === è¾“å‡ºé…ç½® ===
    output_dir: str = "./.benchmarks/refiner"
    save_raw_results: bool = True
    generate_report: bool = True

    # === è¿è¡Œé…ç½® ===
    seed: int = 42
    verbose: bool = True
    timeout: int = 600  # å•ä¸ªæ ·æœ¬è¶…æ—¶ï¼ˆç§’ï¼‰

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "name": self.name,
            "description": self.description,
            "dataset": self.dataset.value if isinstance(self.dataset, Enum) else self.dataset,
            "datasets": self.datasets,
            "dataset_config": self.dataset_config,
            "split": self.split,
            "max_samples": self.max_samples,
            "algorithms": self.algorithms,
            "budget": self.budget,
            "retriever_type": self.retriever_type,
            "top_k": self.top_k,
            "index_path": self.index_path,
            "documents_path": self.documents_path,
            "generator_model": self.generator_model,
            "generator_base_url": self.generator_base_url,
            "output_dir": self.output_dir,
            "save_raw_results": self.save_raw_results,
            "generate_report": self.generate_report,
            "seed": self.seed,
            "verbose": self.verbose,
            "timeout": self.timeout,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "RefinerExperimentConfig":
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        # å¤„ç†æšä¸¾ç±»å‹
        if "dataset" in data and isinstance(data["dataset"], str):
            try:
                data["dataset"] = DatasetType(data["dataset"])
            except ValueError:
                # è‡ªå®šä¹‰æ•°æ®é›†åç§°ï¼Œä¿æŒå­—ç¬¦ä¸²
                pass
        # å¤„ç† datasets å­—æ®µ
        if "datasets" not in data and "dataset" in data:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ datasetsï¼Œä» dataset åˆ›å»º
            dataset_val = data["dataset"]
            if isinstance(dataset_val, DatasetType):
                data["datasets"] = [dataset_val.value]
            else:
                data["datasets"] = [dataset_val]
        return cls(**data)

    @classmethod
    def from_yaml(cls, path: str) -> "RefinerExperimentConfig":
        """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(path) as f:
            data = yaml.safe_load(f)
        return cls.from_dict(data.get("experiment", data))

    def save_yaml(self, path: str) -> None:
        """ä¿å­˜é…ç½®åˆ° YAML æ–‡ä»¶"""
        with open(path, "w") as f:
            yaml.dump({"experiment": self.to_dict()}, f, default_flow_style=False)

    def validate(self) -> list[str]:
        """éªŒè¯é…ç½®ï¼Œè¿”å›é”™è¯¯åˆ—è¡¨"""
        errors = []

        # æ£€æŸ¥ç®—æ³•
        available = RefinerAlgorithm.available()
        for algo in self.algorithms:
            if algo not in available and algo not in [e.value for e in RefinerAlgorithm]:
                errors.append(f"Unknown algorithm: {algo}")

        # æ£€æŸ¥æ•°æ®é›†
        if self.max_samples <= 0:
            errors.append("max_samples must be positive")

        # éªŒè¯ datasets åˆ—è¡¨
        if not self.datasets:
            errors.append("datasets cannot be empty")
        else:
            for ds in self.datasets:
                if ds not in AVAILABLE_DATASETS:
                    errors.append(f"Unknown dataset: {ds}. Available: {AVAILABLE_DATASETS}")

        # æ£€æŸ¥ budget
        if self.budget <= 0:
            errors.append("budget must be positive")

        return errors

    def get_datasets(self) -> list[str]:
        """
        è·å–è¦è¿è¡Œçš„æ•°æ®é›†åˆ—è¡¨

        Returns:
            æ•°æ®é›†åç§°åˆ—è¡¨
        """
        return self.datasets


@dataclass
class AlgorithmMetrics:
    """å•ä¸ªç®—æ³•çš„è¯„æµ‹æŒ‡æ ‡"""

    algorithm: str
    num_samples: int = 0

    # è´¨é‡æŒ‡æ ‡
    avg_f1: float = 0.0
    avg_recall: float = 0.0
    avg_rouge_l: float = 0.0
    avg_accuracy: float = 0.0

    # å‹ç¼©æŒ‡æ ‡
    avg_compression_rate: float = 0.0
    avg_original_tokens: float = 0.0
    avg_compressed_tokens: float = 0.0

    # å»¶è¿ŸæŒ‡æ ‡ï¼ˆç§’ï¼‰
    avg_retrieve_time: float = 0.0
    avg_refine_time: float = 0.0
    avg_generate_time: float = 0.0
    avg_total_time: float = 0.0

    # æ ‡å‡†å·®
    std_f1: float = 0.0
    std_compression_rate: float = 0.0
    std_total_time: float = 0.0

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "algorithm": self.algorithm,
            "num_samples": self.num_samples,
            "quality": {
                "avg_f1": self.avg_f1,
                "avg_recall": self.avg_recall,
                "avg_rouge_l": self.avg_rouge_l,
                "avg_accuracy": self.avg_accuracy,
                "std_f1": self.std_f1,
            },
            "compression": {
                "avg_compression_rate": self.avg_compression_rate,
                "avg_original_tokens": self.avg_original_tokens,
                "avg_compressed_tokens": self.avg_compressed_tokens,
                "std_compression_rate": self.std_compression_rate,
            },
            "latency": {
                "avg_retrieve_time": self.avg_retrieve_time,
                "avg_refine_time": self.avg_refine_time,
                "avg_generate_time": self.avg_generate_time,
                "avg_total_time": self.avg_total_time,
                "std_total_time": self.std_total_time,
            },
        }


@dataclass
class ExperimentResult:
    """å®éªŒç»“æœ"""

    experiment_id: str
    config: dict[str, Any]
    algorithm_metrics: dict[str, AlgorithmMetrics] = field(default_factory=dict)
    raw_results: list[dict[str, Any]] = field(default_factory=list)
    start_time: str = ""
    end_time: str = ""
    duration_seconds: float = 0.0
    success: bool = True
    error: str = ""

    # å¯¹æ¯”ç»“æœ
    best_f1_algorithm: str = ""
    best_compression_algorithm: str = ""
    best_latency_algorithm: str = ""

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "experiment_id": self.experiment_id,
            "config": self.config,
            "algorithm_metrics": {
                name: metrics.to_dict() for name, metrics in self.algorithm_metrics.items()
            },
            "raw_results": self.raw_results if len(self.raw_results) <= 100 else [],
            "start_time": self.start_time,
            "end_time": self.end_time,
            "duration_seconds": self.duration_seconds,
            "success": self.success,
            "error": self.error,
            "summary": {
                "best_f1_algorithm": self.best_f1_algorithm,
                "best_compression_algorithm": self.best_compression_algorithm,
                "best_latency_algorithm": self.best_latency_algorithm,
            },
        }


class BaseRefinerExperiment(ABC):
    """
    Refiner è¯„æµ‹å®éªŒåŸºç±»

    å®šä¹‰å®éªŒç”Ÿå‘½å‘¨æœŸï¼š
    1. prepare() - å‡†å¤‡æ•°æ®å’Œèµ„æº
    2. run() - æ‰§è¡Œå®éªŒ
    3. analyze() - åˆ†æç»“æœ
    4. finalize() - æ¸…ç†å’Œä¿å­˜

    å­ç±»å¿…é¡»å®ç° run() æ–¹æ³•ã€‚
    """

    def __init__(self, config: RefinerExperimentConfig):
        """
        åˆå§‹åŒ–å®éªŒ

        Args:
            config: å®éªŒé…ç½®
        """
        self.config = config
        self.experiment_id = f"{config.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.output_dir = Path(config.output_dir) / self.experiment_id
        self.result: ExperimentResult | None = None

    def _log(self, message: str) -> None:
        """æ—¥å¿—è¾“å‡º"""
        if self.config.verbose:
            print(message)

    def prepare(self) -> None:
        """
        å‡†å¤‡å®éªŒ

        - éªŒè¯é…ç½®
        - åˆ›å»ºè¾“å‡ºç›®å½•
        - åŠ è½½æ•°æ®æº
        """
        # éªŒè¯é…ç½®
        errors = self.config.validate()
        if errors:
            raise ValueError(f"Configuration errors: {errors}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜é…ç½®
        config_path = self.output_dir / "config.yaml"
        self.config.save_yaml(str(config_path))

        self._log(f"ğŸ“ Output directory: {self.output_dir}")
        self._log(f"ğŸ“ Config saved to: {config_path}")

    @abstractmethod
    def run(self) -> ExperimentResult:
        """
        è¿è¡Œå®éªŒ

        Returns:
            ExperimentResult åŒ…å«æ‰€æœ‰ç®—æ³•çš„è¯„æµ‹ç»“æœ
        """
        pass

    def analyze(self, result: ExperimentResult) -> ExperimentResult:
        """
        åˆ†æå®éªŒç»“æœï¼Œç¡®å®šæœ€ä½³ç®—æ³•

        Args:
            result: åŸå§‹å®éªŒç»“æœ

        Returns:
            æ·»åŠ äº†åˆ†æç»“è®ºçš„å®éªŒç»“æœ
        """
        if not result.algorithm_metrics:
            return result

        # æ‰¾å‡ºæœ€ä½³ F1
        best_f1 = max(
            result.algorithm_metrics.items(),
            key=lambda x: x[1].avg_f1,
        )
        result.best_f1_algorithm = best_f1[0]

        # æ‰¾å‡ºæœ€ä½³å‹ç¼©ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        best_compression = max(
            result.algorithm_metrics.items(),
            key=lambda x: x[1].avg_compression_rate,
        )
        result.best_compression_algorithm = best_compression[0]

        # æ‰¾å‡ºæœ€ä½å»¶è¿Ÿ
        best_latency = min(
            result.algorithm_metrics.items(),
            key=lambda x: x[1].avg_total_time if x[1].avg_total_time > 0 else float("inf"),
        )
        result.best_latency_algorithm = best_latency[0]

        return result

    def finalize(self, result: ExperimentResult) -> None:
        """
        å®Œæˆå®éªŒ

        - ä¿å­˜ç»“æœ
        - ç”ŸæˆæŠ¥å‘Š
        - æ¸…ç†èµ„æº
        """
        import json

        # ä¿å­˜ JSON ç»“æœ
        result_path = self.output_dir / "results.json"
        with open(result_path, "w") as f:
            json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)
        self._log(f"ğŸ’¾ Results saved to: {result_path}")

        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        if self.config.generate_report:
            report_path = self.output_dir / "report.md"
            self._generate_report(result, report_path)
            self._log(f"ğŸ“Š Report saved to: {report_path}")

    def _generate_report(self, result: ExperimentResult, path: Path) -> None:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        lines = [
            "# Refiner Benchmark Report",
            "",
            f"**Experiment ID**: {result.experiment_id}",
            f"**Duration**: {result.duration_seconds:.1f}s",
            f"**Samples**: {self.config.max_samples}",
            f"**Dataset**: {self.config.dataset.value}",
            "",
            "## Summary",
            "",
            "| Best For | Algorithm |",
            "|----------|-----------|",
            f"| F1 Score | {result.best_f1_algorithm} |",
            f"| Compression | {result.best_compression_algorithm} |",
            f"| Latency | {result.best_latency_algorithm} |",
            "",
            "## Algorithm Comparison",
            "",
            "| Algorithm | F1 | Compression | Latency (s) |",
            "|-----------|-----|-------------|-------------|",
        ]

        for name, metrics in result.algorithm_metrics.items():
            lines.append(
                f"| {name} | {metrics.avg_f1:.4f} | "
                f"{metrics.avg_compression_rate:.2f}x | {metrics.avg_total_time:.2f} |"
            )

        lines.extend(
            [
                "",
                "## Configuration",
                "",
                "```yaml",
            ]
        )

        # æ·»åŠ é…ç½®
        import yaml

        lines.append(yaml.dump(result.config, default_flow_style=False))
        lines.append("```")

        with open(path, "w") as f:
            f.write("\n".join(lines))

    def run_full(self) -> ExperimentResult:
        """
        è¿è¡Œå®Œæ•´å®éªŒæµç¨‹

        Returns:
            ExperimentResult
        """
        self._log(f"\n{'=' * 60}")
        self._log(f"ğŸš€ Starting Refiner Benchmark: {self.config.name}")
        self._log(f"{'=' * 60}")

        try:
            # 1. å‡†å¤‡
            self._log("\nğŸ“¦ Preparing experiment...")
            self.prepare()

            # 2. è¿è¡Œ
            self._log("\nâ–¶ï¸  Running experiment...")
            result = self.run()

            # 3. åˆ†æ
            self._log("\nğŸ” Analyzing results...")
            result = self.analyze(result)

            # 4. å®Œæˆ
            self._log("\nğŸ’¾ Finalizing...")
            self.finalize(result)

            self._log(f"\n{'=' * 60}")
            self._log("âœ… Experiment completed successfully")
            self._log(f"{'=' * 60}")

            return result

        except Exception as e:
            self._log(f"\nâŒ Experiment failed: {e}")
            import traceback

            traceback.print_exc()

            return ExperimentResult(
                experiment_id=self.experiment_id,
                config=self.config.to_dict(),
                success=False,
                error=str(e),
            )
