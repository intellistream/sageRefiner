"""
Refiner Experiment Runner
=========================

ç»Ÿä¸€çš„å®éªŒè¿è¡Œå™¨ï¼Œæ”¯æŒå‘½ä»¤è¡Œå’Œç¼–ç¨‹æ–¹å¼è¿è¡Œå®éªŒã€‚
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING, Any

from benchmarks.experiments.base_experiment import (
    BaseRefinerExperiment,
    ExperimentResult,
    RefinerExperimentConfig,
)
from benchmarks.experiments.comparison_experiment import (
    ComparisonExperiment,
    CompressionExperiment,
    LatencyExperiment,
    QualityExperiment,
)

if TYPE_CHECKING:
    from benchmarks.experiments.comparison_experiment import (
        MultiDatasetExperimentResult,
    )


class RefinerExperimentRunner:
    """
    Refiner å®éªŒè¿è¡Œå™¨

    æä¾›ç»Ÿä¸€çš„æ¥å£è¿è¡Œå„ç§ç±»å‹çš„ Refiner è¯„æµ‹å®éªŒã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        # æ–¹å¼1: ä»é…ç½®æ–‡ä»¶è¿è¡Œ
        runner = RefinerExperimentRunner()
        result = runner.run_from_config("config.yaml")

        # æ–¹å¼2: ä»é…ç½®å¯¹è±¡è¿è¡Œ
        config = RefinerExperimentConfig(
            name="my_experiment",
            algorithms=["baseline", "longrefiner"],
        )
        result = runner.run(config)

        # æ–¹å¼3: å¿«é€Ÿå¯¹æ¯”
        result = runner.quick_compare(
            algorithms=["baseline", "longrefiner", "reform"],
            max_samples=50,
        )
    """

    EXPERIMENT_TYPES: dict[str, type[BaseRefinerExperiment]] = {
        "comparison": ComparisonExperiment,
        "quality": QualityExperiment,
        "latency": LatencyExperiment,
        "compression": CompressionExperiment,
    }

    def __init__(self, verbose: bool = True):
        """
        åˆå§‹åŒ–è¿è¡Œå™¨

        Args:
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        self.verbose = verbose

    def _log(self, message: str) -> None:
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(message)

    def run(
        self,
        config: RefinerExperimentConfig,
        experiment_type: str = "comparison",
    ) -> ExperimentResult:
        """
        è¿è¡Œå®éªŒ

        Args:
            config: å®éªŒé…ç½®
            experiment_type: å®éªŒç±»å‹ (comparison, quality, latency, compression)

        Returns:
            ExperimentResult
        """
        if experiment_type not in self.EXPERIMENT_TYPES:
            raise ValueError(
                f"Unknown experiment type: {experiment_type}. "
                f"Available: {list(self.EXPERIMENT_TYPES.keys())}"
            )

        experiment_class = self.EXPERIMENT_TYPES[experiment_type]
        experiment = experiment_class(config)

        return experiment.run_full()

    def run_from_config(
        self,
        config_path: str,
        experiment_type: str = "comparison",
    ) -> ExperimentResult:
        """
        ä»é…ç½®æ–‡ä»¶è¿è¡Œå®éªŒ

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ (YAML)
            experiment_type: å®éªŒç±»å‹

        Returns:
            ExperimentResult
        """
        config = RefinerExperimentConfig.from_yaml(config_path)
        return self.run(config, experiment_type)

    def run_from_dict(
        self,
        config_dict: dict[str, Any],
        experiment_type: str = "comparison",
    ) -> ExperimentResult:
        """
        ä»å­—å…¸è¿è¡Œå®éªŒ

        Args:
            config_dict: é…ç½®å­—å…¸
            experiment_type: å®éªŒç±»å‹

        Returns:
            ExperimentResult
        """
        config = RefinerExperimentConfig.from_dict(config_dict)
        return self.run(config, experiment_type)

    def quick_compare(
        self,
        algorithms: list[str] | None = None,
        max_samples: int = 50,
        budget: int = 2048,
        datasets: list[str] | None = None,
        dataset: str | None = None,  # å‘åå…¼å®¹
        output_dir: str = "./.benchmarks/refiner",
    ) -> ExperimentResult:
        """
        å¿«é€Ÿå¯¹æ¯”å¤šç§ç®—æ³•

        Args:
            algorithms: è¦å¯¹æ¯”çš„ç®—æ³•åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰å·²å®ç°ç®—æ³•
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            budget: Token é¢„ç®—
            datasets: æ•°æ®é›†åˆ—è¡¨
            dataset: å•ä¸ªæ•°æ®é›†åç§° (å‘åå…¼å®¹)
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ExperimentResult
        """
        if algorithms is None:
            algorithms = ["baseline", "longrefiner", "reform", "provence"]

        # å¤„ç†æ•°æ®é›†å‚æ•°ï¼šæ”¯æŒæ–°çš„ datasets å’Œæ—§çš„ dataset
        if datasets is None:
            datasets = [dataset] if dataset is not None else ["nq"]

        config = RefinerExperimentConfig(
            name="quick_comparison",
            algorithms=algorithms,
            max_samples=max_samples,
            budget=budget,
            datasets=datasets,
            dataset_config=datasets[0],  # ä¿æŒå‘åå…¼å®¹
            output_dir=output_dir,
            verbose=self.verbose,
        )

        return self.run(config, "comparison")

    def compare_budgets(
        self,
        algorithm: str,
        budgets: list[int],
        max_samples: int = 50,
        output_dir: str = "./.benchmarks/refiner",
    ) -> dict[int, ExperimentResult]:
        """
        å¯¹æ¯”ä¸åŒ budget ä¸‹çš„è¡¨ç°

        Args:
            algorithm: ç®—æ³•åç§°
            budgets: è¦æµ‹è¯•çš„ budget åˆ—è¡¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            {budget: ExperimentResult} å­—å…¸
        """
        results = {}

        for budget in budgets:
            self._log(f"\nğŸ“Š Testing budget: {budget}")

            config = RefinerExperimentConfig(
                name=f"budget_sweep_{algorithm}_{budget}",
                algorithms=[algorithm],
                max_samples=max_samples,
                budget=budget,
                output_dir=output_dir,
                verbose=self.verbose,
            )

            result = self.run(config, "compression")
            results[budget] = result

        return results

    def run_sweep(
        self,
        algorithms: list[str],
        budgets: list[int],
        max_samples: int = 50,
        output_dir: str = "./.benchmarks/refiner",
    ) -> dict[str, dict[int, ExperimentResult]]:
        """
        è¿è¡Œå®Œæ•´çš„å‚æ•°æ‰«æ

        Args:
            algorithms: ç®—æ³•åˆ—è¡¨
            budgets: budget åˆ—è¡¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            {algorithm: {budget: ExperimentResult}} åµŒå¥—å­—å…¸
        """
        all_results = {}

        for algorithm in algorithms:
            self._log(f"\n{'=' * 60}")
            self._log(f"ğŸ”§ Sweeping algorithm: {algorithm}")
            self._log(f"{'=' * 60}")

            all_results[algorithm] = self.compare_budgets(
                algorithm=algorithm,
                budgets=budgets,
                max_samples=max_samples,
                output_dir=output_dir,
            )

        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_path = Path(output_dir) / "sweep_summary.json"
        summary = {
            algo: {str(budget): result.to_dict() for budget, result in budget_results.items()}
            for algo, budget_results in all_results.items()
        }

        summary_path.parent.mkdir(parents=True, exist_ok=True)
        with open(summary_path, "w") as f:
            json.dump(summary, f, indent=2)

        self._log(f"\nğŸ’¾ Sweep summary saved to: {summary_path}")

        return all_results

    @staticmethod
    def print_comparison_table(result: ExperimentResult) -> None:
        """
        æ‰“å°å¯¹æ¯”è¡¨æ ¼

        Args:
            result: å®éªŒç»“æœ
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šæ•°æ®é›†ä¿¡æ¯
        config = result.config
        datasets = config.get("datasets", [config.get("dataset_config", "unknown")])

        print("\n" + "=" * 80)
        print("                    Refiner Algorithm Comparison")
        if len(datasets) > 1:
            print(f"                    Datasets: {', '.join(datasets)}")
        print("=" * 80)

        headers = ["Algorithm", "F1 Score", "Compression", "Latency (s)", "Samples"]
        header_line = "| " + " | ".join(f"{h:^12}" for h in headers) + " |"
        print(header_line)
        print("|" + "|".join("-" * 14 for _ in headers) + "|")

        for name, metrics in result.algorithm_metrics.items():
            row = [
                name[:12],
                f"{metrics.avg_f1:.4f}",
                f"{metrics.avg_compression_rate:.2f}x",
                f"{metrics.avg_total_time:.2f}",
                str(metrics.num_samples),
            ]
            row_line = "| " + " | ".join(f"{v:^12}" for v in row) + " |"
            print(row_line)

        print("=" * 80)

        # æ‰“å°æœ€ä½³ç®—æ³•
        print(f"\nğŸ† Best F1: {result.best_f1_algorithm}")
        print(f"ğŸ† Best Compression: {result.best_compression_algorithm}")
        print(f"ğŸ† Best Latency: {result.best_latency_algorithm}")

    @staticmethod
    def print_multi_dataset_table(
        result: MultiDatasetExperimentResult,
    ) -> None:
        """
        æ‰“å°å¤šæ•°æ®é›†å¯¹æ¯”è¡¨æ ¼

        Args:
            result: å¤šæ•°æ®é›†å®éªŒç»“æœ
        """
        from benchmarks.experiments.comparison_experiment import (
            MultiDatasetExperimentResult as MultiDatasetResult,
        )

        if not isinstance(result, MultiDatasetResult):
            # å›é€€åˆ°å•æ•°æ®é›†è¡¨æ ¼
            RefinerExperimentRunner.print_comparison_table(result)
            return

        # æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„ç»“æœ
        for dataset, ds_result in result.dataset_results.items():
            print(f"\n{'=' * 60}")
            print(f"                Dataset: {dataset}")
            print("=" * 60)

            headers = ["Algorithm", "F1", "Compression", "Latency"]
            print("| " + " | ".join(f"{h:^12}" for h in headers) + " |")
            print("|" + "|".join("-" * 14 for _ in headers) + "|")

            for name, metrics in ds_result.algorithm_metrics.items():
                row = [
                    name[:12],
                    f"{metrics.avg_f1:.4f}",
                    f"{metrics.avg_compression_rate:.2f}x",
                    f"{metrics.avg_total_time:.2f}s",
                ]
                print("| " + " | ".join(f"{v:^12}" for v in row) + " |")

        # æ‰“å°èšåˆç»“æœ
        print(f"\n{'=' * 60}")
        print("                Aggregated Results (Cross-Dataset Average)")
        print("=" * 60)

        headers = ["Algorithm", "F1", "Compression", "Latency", "Total Samples"]
        print("| " + " | ".join(f"{h:^12}" for h in headers) + " |")
        print("|" + "|".join("-" * 14 for _ in headers) + "|")

        for name, metrics in result.aggregated_metrics.items():
            row = [
                name[:12],
                f"{metrics.avg_f1:.4f}",
                f"{metrics.avg_compression_rate:.2f}x",
                f"{metrics.avg_total_time:.2f}s",
                str(metrics.num_samples),
            ]
            print("| " + " | ".join(f"{v:^12}" for v in row) + " |")

        print("=" * 60)
        print(f"\nğŸ† Best F1: {result.best_f1_algorithm}")
        print(f"ğŸ† Best Compression: {result.best_compression_algorithm}")
        print(f"ğŸ† Best Latency: {result.best_latency_algorithm}")

    @staticmethod
    def generate_latex_table(result: ExperimentResult) -> str:
        """
        ç”Ÿæˆ LaTeX è¡¨æ ¼

        Args:
            result: å®éªŒç»“æœ

        Returns:
            LaTeX è¡¨æ ¼å­—ç¬¦ä¸²
        """
        lines = [
            r"\begin{table}[h]",
            r"\centering",
            r"\caption{Refiner Algorithm Comparison}",
            r"\label{tab:refiner-comparison}",
            r"\begin{tabular}{lcccc}",
            r"\toprule",
            r"Algorithm & F1 Score & Compression & Latency (s) & Samples \\",
            r"\midrule",
        ]

        for name, metrics in result.algorithm_metrics.items():
            # æ ‡è®°æœ€ä½³å€¼
            f1_str = f"{metrics.avg_f1:.4f}"
            comp_str = f"{metrics.avg_compression_rate:.2f}x"
            lat_str = f"{metrics.avg_total_time:.2f}"

            if name == result.best_f1_algorithm:
                f1_str = r"\textbf{" + f1_str + "}"
            if name == result.best_compression_algorithm:
                comp_str = r"\textbf{" + comp_str + "}"
            if name == result.best_latency_algorithm:
                lat_str = r"\textbf{" + lat_str + "}"

            lines.append(f"{name} & {f1_str} & {comp_str} & {lat_str} & {metrics.num_samples} \\\\")

        lines.extend(
            [
                r"\bottomrule",
                r"\end{tabular}",
                r"\end{table}",
            ]
        )

        return "\n".join(lines)
