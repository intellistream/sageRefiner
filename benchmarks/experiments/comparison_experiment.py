"""
Refiner Comparison Experiment
=============================

å¤šç®—æ³•å¯¹æ¯”è¯„æµ‹å®éªŒï¼Œè¿è¡Œå¤šç§ Refiner ç®—æ³•å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šã€‚
æ”¯æŒå¤šæ•°æ®é›†æ‰¹é‡è¿è¡Œã€‚

é‡è¦ï¼šæ­¤æ¨¡å—è°ƒç”¨çœŸå®çš„ RAG Pipeline è¿›è¡Œè¯„æµ‹ï¼Œé€šè¿‡ ResultsCollector æ”¶é›†ç»“æœã€‚
"""

import statistics
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

from sage.common.utils.config.loader import load_config

from benchmarks.experiments.base_experiment import (
    AlgorithmMetrics,
    BaseRefinerExperiment,
    ExperimentResult,
    RefinerExperimentConfig,
)
from benchmarks.experiments.results_collector import (
    ResultsCollector,
)


@dataclass
class DatasetResult:
    """å•ä¸ªæ•°æ®é›†çš„è¯„æµ‹ç»“æœ"""

    dataset: str
    algorithm_metrics: dict[str, AlgorithmMetrics] = field(default_factory=dict)
    raw_results: list[dict[str, Any]] = field(default_factory=list)
    success: bool = True
    error: str = ""

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "dataset": self.dataset,
            "algorithm_metrics": {
                name: metrics.to_dict() for name, metrics in self.algorithm_metrics.items()
            },
            "raw_results": self.raw_results if len(self.raw_results) <= 100 else [],
            "success": self.success,
            "error": self.error,
        }


@dataclass
class MultiDatasetExperimentResult:
    """
    å¤šæ•°æ®é›†å®éªŒç»“æœ

    æ‰©å±• ExperimentResult ä»¥æ”¯æŒæŒ‰æ•°æ®é›†åˆ†ç»„çš„ç»“æœã€‚
    """

    experiment_id: str
    config: dict[str, Any]
    dataset_results: dict[str, DatasetResult] = field(default_factory=dict)
    aggregated_metrics: dict[str, AlgorithmMetrics] = field(default_factory=dict)
    start_time: str = ""
    end_time: str = ""
    duration_seconds: float = 0.0
    success: bool = True
    error: str = ""

    # å¯¹æ¯”ç»“æœ
    best_f1_algorithm: str = ""
    best_compression_algorithm: str = ""
    best_latency_algorithm: str = ""

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "experiment_id": self.experiment_id,
            "config": self.config,
            "datasets": {name: result.to_dict() for name, result in self.dataset_results.items()},
            "aggregated": {
                name: metrics.to_dict() for name, metrics in self.aggregated_metrics.items()
            },
            "start_time": self.start_time,
            "end_time": self.end_time,
            "duration_seconds": self.duration_seconds,
            "success": self.success,
            "error": self.error,
            "summary": {
                "best_f1_algorithm": self.best_f1_algorithm,
                "best_compression_algorithm": self.best_compression_algorithm,
                "best_latency_algorithm": self.best_latency_algorithm,
            },
        }

    def to_experiment_result(self) -> ExperimentResult:
        """
        è½¬æ¢ä¸ºæ ‡å‡† ExperimentResult ä»¥ä¿æŒå‘åå…¼å®¹æ€§ã€‚

        ä½¿ç”¨èšåˆæŒ‡æ ‡ä½œä¸º algorithm_metricsã€‚
        """
        all_raw_results = []
        for ds_result in self.dataset_results.values():
            for sample in ds_result.raw_results:
                sample["dataset"] = ds_result.dataset
                all_raw_results.append(sample)

        return ExperimentResult(
            experiment_id=self.experiment_id,
            config=self.config,
            algorithm_metrics=self.aggregated_metrics,
            raw_results=all_raw_results,
            start_time=self.start_time,
            end_time=self.end_time,
            duration_seconds=self.duration_seconds,
            success=self.success,
            error=self.error,
            best_f1_algorithm=self.best_f1_algorithm,
            best_compression_algorithm=self.best_compression_algorithm,
            best_latency_algorithm=self.best_latency_algorithm,
        )


class ComparisonExperiment(BaseRefinerExperiment):
    """
    å¤šç®—æ³•å¯¹æ¯”å®éªŒ

    å¯¹å¤šç§ Refiner ç®—æ³•åœ¨åŒä¸€æ•°æ®é›†ä¸Šè¿›è¡Œè¯„æµ‹ï¼Œ
    æ”¶é›†è´¨é‡ã€å‹ç¼©ç‡ã€å»¶è¿Ÿç­‰æŒ‡æ ‡å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šã€‚

    æ”¯æŒå¤šæ•°æ®é›†æ‰¹é‡è¿è¡Œã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        config = RefinerExperimentConfig(
            name="algorithm_comparison",
            algorithms=["baseline", "longrefiner", "reform", "provence"],
            datasets=["nq", "hotpotqa", "2wikimultihopqa"],
            max_samples=100,
            budget=2048,
        )
        experiment = ComparisonExperiment(config)
        result = experiment.run_full()
    """

    def __init__(self, config: RefinerExperimentConfig):
        super().__init__(config)
        self.sample_results: dict[str, list[dict[str, Any]]] = {}
        # å¤šæ•°æ®é›†ç»“æœå­˜å‚¨
        self.multi_dataset_result: MultiDatasetExperimentResult | None = None

    def run(self) -> ExperimentResult:
        """
        è¿è¡Œå¯¹æ¯”å®éªŒ

        å¯¹æ¯ä¸ªæ•°æ®é›†å’Œæ¯ç§ç®—æ³•ï¼š
        1. åŠ è½½å¯¹åº”çš„ Pipeline é…ç½®
        2. è¿è¡Œ Pipeline
        3. æ”¶é›†è¯„æµ‹æŒ‡æ ‡

        Returns:
            ExperimentResult åŒ…å«æ‰€æœ‰ç®—æ³•çš„å¯¹æ¯”ç»“æœï¼ˆèšåˆï¼‰
        """
        from datetime import timezone

        start_time = datetime.now(tz=timezone.utc)

        # è·å–è¦è¿è¡Œçš„æ•°æ®é›†åˆ—è¡¨
        datasets = self.config.get_datasets()

        # åˆå§‹åŒ–å¤šæ•°æ®é›†ç»“æœ
        self.multi_dataset_result = MultiDatasetExperimentResult(
            experiment_id=self.experiment_id,
            config=self.config.to_dict(),
            start_time=start_time.isoformat(),
        )

        # å¯¹æ¯ä¸ªæ•°æ®é›†è¿è¡Œå®éªŒ
        for dataset in datasets:
            self._log(f"\n{'=' * 50}")
            self._log(f"ğŸ“Š Running on dataset: {dataset}")
            self._log(f"{'=' * 50}")

            dataset_result = self._run_on_dataset(dataset)
            self.multi_dataset_result.dataset_results[dataset] = dataset_result

        # èšåˆè·¨æ•°æ®é›†ç»“æœ
        self._aggregate_results()

        end_time = datetime.now(tz=timezone.utc)
        self.multi_dataset_result.end_time = end_time.isoformat()
        self.multi_dataset_result.duration_seconds = (end_time - start_time).total_seconds()

        # ä¿å­˜å¤šæ•°æ®é›†ç»“æœ
        self._save_multi_dataset_result()

        # è¿”å›æ ‡å‡† ExperimentResultï¼ˆå‘åå…¼å®¹ï¼‰
        return self.multi_dataset_result.to_experiment_result()

    def _run_on_dataset(self, dataset: str) -> DatasetResult:
        """
        åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œæ‰€æœ‰ç®—æ³•çš„è¯„æµ‹

        Args:
            dataset: æ•°æ®é›†åç§°

        Returns:
            DatasetResult è¯¥æ•°æ®é›†ä¸Šæ‰€æœ‰ç®—æ³•çš„è¯„æµ‹ç»“æœ
        """
        result = DatasetResult(dataset=dataset)

        for algorithm in self.config.algorithms:
            self._log(f"\n{'â”€' * 40}")
            self._log(f"ğŸ”§ Running algorithm: {algorithm} on {dataset}")
            self._log(f"{'â”€' * 40}")

            try:
                metrics = self._run_algorithm(algorithm, dataset)
                result.algorithm_metrics[algorithm] = metrics
                self._log(
                    f"   âœ… Completed: F1={metrics.avg_f1:.4f}, "
                    f"Compression={metrics.avg_compression_rate:.2f}x"
                )
            except Exception as e:
                self._log(f"   âŒ Failed: {e}")
                # è®°å½•å¤±è´¥ä½†ç»§ç»­å…¶ä»–ç®—æ³•
                result.algorithm_metrics[algorithm] = AlgorithmMetrics(
                    algorithm=algorithm,
                    num_samples=0,
                )

        # æ”¶é›†åŸå§‹ç»“æœ
        if self.config.save_raw_results:
            for algo, samples in self.sample_results.items():
                for sample in samples:
                    sample["algorithm"] = algo
                    sample["dataset"] = dataset
                    result.raw_results.append(sample)

        # æ¸…ç©ºå•æ•°æ®é›†çš„ä¸´æ—¶ç»“æœ
        self.sample_results.clear()

        return result

    def _aggregate_results(self) -> None:
        """
        èšåˆè·¨æ•°æ®é›†çš„ç»“æœ

        è®¡ç®—æ¯ä¸ªç®—æ³•åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡æ€§èƒ½ã€‚
        """
        if self.multi_dataset_result is None:
            return

        # æ”¶é›†æ¯ä¸ªç®—æ³•åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æŒ‡æ ‡
        algo_metrics_collection: dict[str, dict[str, list[float]]] = {}

        for ds_result in self.multi_dataset_result.dataset_results.values():
            for algo, metrics in ds_result.algorithm_metrics.items():
                if algo not in algo_metrics_collection:
                    algo_metrics_collection[algo] = {
                        "f1": [],
                        "compression_rate": [],
                        "total_time": [],
                        "retrieve_time": [],
                        "refine_time": [],
                        "generate_time": [],
                        "num_samples": [],
                    }

                if metrics.num_samples > 0:
                    algo_metrics_collection[algo]["f1"].append(metrics.avg_f1)
                    algo_metrics_collection[algo]["compression_rate"].append(
                        metrics.avg_compression_rate
                    )
                    algo_metrics_collection[algo]["total_time"].append(metrics.avg_total_time)
                    algo_metrics_collection[algo]["retrieve_time"].append(metrics.avg_retrieve_time)
                    algo_metrics_collection[algo]["refine_time"].append(metrics.avg_refine_time)
                    algo_metrics_collection[algo]["generate_time"].append(metrics.avg_generate_time)
                    algo_metrics_collection[algo]["num_samples"].append(metrics.num_samples)

        # è®¡ç®—èšåˆæŒ‡æ ‡
        for algo, metrics_dict in algo_metrics_collection.items():
            if not metrics_dict["f1"]:
                continue

            aggregated = AlgorithmMetrics(
                algorithm=algo,
                num_samples=int(sum(metrics_dict["num_samples"])),
                avg_f1=statistics.mean(metrics_dict["f1"]),
                avg_compression_rate=statistics.mean(metrics_dict["compression_rate"]),
                avg_total_time=statistics.mean(metrics_dict["total_time"]),
                avg_retrieve_time=statistics.mean(metrics_dict["retrieve_time"]),
                avg_refine_time=statistics.mean(metrics_dict["refine_time"]),
                avg_generate_time=statistics.mean(metrics_dict["generate_time"]),
            )

            # è®¡ç®—æ ‡å‡†å·®
            if len(metrics_dict["f1"]) > 1:
                aggregated.std_f1 = statistics.stdev(metrics_dict["f1"])
                aggregated.std_compression_rate = statistics.stdev(metrics_dict["compression_rate"])
                aggregated.std_total_time = statistics.stdev(metrics_dict["total_time"])

            self.multi_dataset_result.aggregated_metrics[algo] = aggregated

        # ç¡®å®šæœ€ä½³ç®—æ³•
        if self.multi_dataset_result.aggregated_metrics:
            best_f1 = max(
                self.multi_dataset_result.aggregated_metrics.items(),
                key=lambda x: x[1].avg_f1,
            )
            self.multi_dataset_result.best_f1_algorithm = best_f1[0]

            best_compression = max(
                self.multi_dataset_result.aggregated_metrics.items(),
                key=lambda x: x[1].avg_compression_rate,
            )
            self.multi_dataset_result.best_compression_algorithm = best_compression[0]

            best_latency = min(
                self.multi_dataset_result.aggregated_metrics.items(),
                key=lambda x: x[1].avg_total_time if x[1].avg_total_time > 0 else float("inf"),
            )
            self.multi_dataset_result.best_latency_algorithm = best_latency[0]

    def _save_multi_dataset_result(self) -> None:
        """ä¿å­˜å¤šæ•°æ®é›†ç»“æœåˆ°å•ç‹¬çš„ JSON æ–‡ä»¶"""
        import json

        if self.multi_dataset_result is None:
            return

        result_path = self.output_dir / "multi_dataset_results.json"
        with open(result_path, "w") as f:
            json.dump(self.multi_dataset_result.to_dict(), f, indent=2, ensure_ascii=False)
        self._log(f"ğŸ’¾ Multi-dataset results saved to: {result_path}")

    def _run_algorithm(self, algorithm: str, dataset: str = "") -> AlgorithmMetrics:
        """
        è¿è¡Œå•ä¸ªç®—æ³•çš„è¯„æµ‹

        Args:
            algorithm: ç®—æ³•åç§°
            dataset: æ•°æ®é›†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            AlgorithmMetrics è¯¥ç®—æ³•çš„è¯„æµ‹æŒ‡æ ‡
        """
        # æ”¶é›†æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
        f1_scores: list[float] = []
        compression_rates: list[float] = []
        original_tokens_list: list[float] = []
        compressed_tokens_list: list[float] = []
        retrieve_times: list[float] = []
        refine_times: list[float] = []
        generate_times: list[float] = []
        total_times: list[float] = []

        # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿè¿è¡Œ Pipeline å¹¶æ”¶é›†ç»“æœ
        # å®é™…å®ç°ä¸­ä¼šè°ƒç”¨å¯¹åº”çš„ Pipeline
        sample_results = self._execute_pipeline(algorithm, dataset)
        self.sample_results[algorithm] = sample_results

        for sample in sample_results:
            if "f1" in sample:
                f1_scores.append(sample["f1"])
            if "compression_rate" in sample:
                compression_rates.append(sample["compression_rate"])
            if "original_tokens" in sample:
                original_tokens_list.append(sample["original_tokens"])
            if "compressed_tokens" in sample:
                compressed_tokens_list.append(sample["compressed_tokens"])
            if "retrieve_time" in sample:
                retrieve_times.append(sample["retrieve_time"])
            if "refine_time" in sample:
                refine_times.append(sample["refine_time"])
            if "generate_time" in sample:
                generate_times.append(sample["generate_time"])
            if "total_time" in sample:
                total_times.append(sample["total_time"])

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        metrics = AlgorithmMetrics(
            algorithm=algorithm,
            num_samples=len(sample_results),
        )

        if f1_scores:
            metrics.avg_f1 = statistics.mean(f1_scores)
            metrics.std_f1 = statistics.stdev(f1_scores) if len(f1_scores) > 1 else 0.0

        if compression_rates:
            metrics.avg_compression_rate = statistics.mean(compression_rates)
            metrics.std_compression_rate = (
                statistics.stdev(compression_rates) if len(compression_rates) > 1 else 0.0
            )

        if original_tokens_list:
            metrics.avg_original_tokens = statistics.mean(original_tokens_list)

        if compressed_tokens_list:
            metrics.avg_compressed_tokens = statistics.mean(compressed_tokens_list)

        if retrieve_times:
            metrics.avg_retrieve_time = statistics.mean(retrieve_times)

        if refine_times:
            metrics.avg_refine_time = statistics.mean(refine_times)

        if generate_times:
            metrics.avg_generate_time = statistics.mean(generate_times)

        if total_times:
            metrics.avg_total_time = statistics.mean(total_times)
            metrics.std_total_time = statistics.stdev(total_times) if len(total_times) > 1 else 0.0

        return metrics

    def _execute_pipeline(self, algorithm: str, dataset: str = "") -> list[dict[str, Any]]:
        """
        æ‰§è¡ŒçœŸå® Pipeline å¹¶æ”¶é›†ç»“æœ

        é€šè¿‡ ResultsCollector æ”¶é›†è¯„æµ‹ Operators äº§ç”Ÿçš„æŒ‡æ ‡ã€‚

        Args:
            algorithm: ç®—æ³•åç§°
            dataset: æ•°æ®é›†åç§°

        Returns:
            æ¯ä¸ªæ ·æœ¬çš„è¯„æµ‹ç»“æœåˆ—è¡¨
        """
        dataset_info = f" ({dataset})" if dataset else ""
        self._log(f"   ğŸ“Š Running real pipeline for {algorithm}{dataset_info}...")

        # 1. åŠ è½½å¹¶ä¿®æ”¹é…ç½®
        config = self._load_and_modify_config(algorithm, dataset)

        # 2. é‡ç½® ResultsCollector
        collector = ResultsCollector()
        collector.reset()
        collector.set_metadata(
            algorithm=algorithm,
            dataset=dataset,
            max_samples=self.config.max_samples,
        )

        # 3. è¿è¡Œ Pipeline
        try:
            self._run_pipeline_module(algorithm, config)
        except Exception as e:
            self._log(f"   âš ï¸ Pipeline error: {e}")
            # è¿”å›ç©ºç»“æœ
            return []

        # 4. ä» ResultsCollector è·å–ç»“æœ
        results = collector.get_results()
        self._log(f"   âœ… Collected {len(results)} sample results")

        return list(results)

    def _load_and_modify_config(self, algorithm: str, dataset: str = "") -> dict[str, Any]:
        """
        åŠ è½½ç®—æ³•é…ç½®å¹¶ä¿®æ”¹å®éªŒå‚æ•°

        Args:
            algorithm: ç®—æ³•åç§°
            dataset: æ•°æ®é›†åç§°

        Returns:
            ä¿®æ”¹åçš„é…ç½®å­—å…¸
        """
        # é…ç½®æ–‡ä»¶è·¯å¾„
        config_dir = Path(__file__).parent.parent / "config"
        config_filename = f"config_{algorithm}.yaml"
        config_path = config_dir / config_filename

        if not config_path.exists():
            raise FileNotFoundError(
                f"Config file not found: {config_path}. "
                f"Please create config_{algorithm}.yaml for algorithm '{algorithm}'."
            )

        # åŠ è½½é…ç½®
        config: dict[str, Any] = load_config(str(config_path))

        # ä¿®æ”¹ source.max_samples
        if "source" in config:
            config["source"]["max_samples"] = self.config.max_samples

        # ä¿®æ”¹ source.hf_dataset_configï¼ˆå¦‚æœæŒ‡å®šäº†æ•°æ®é›†ï¼‰
        if dataset and "source" in config:
            config["source"]["hf_dataset_config"] = dataset
            self._log(f"   ğŸ“ Using dataset: {dataset}")

        return config

    def _run_pipeline_module(self, algorithm: str, config: dict[str, Any]) -> None:
        """
        è¿è¡ŒæŒ‡å®šç®—æ³•çš„ Pipeline

        æ ¹æ®ç®—æ³•åç§°åŠ¨æ€å¯¼å…¥å¯¹åº”çš„ Pipeline æ¨¡å—å¹¶æ‰§è¡Œã€‚
        ä½¿ç”¨ time.sleep() ç­‰å¾… Pipeline å®Œæˆã€‚

        Args:
            algorithm: ç®—æ³•åç§°
            config: Pipeline é…ç½®

        Raises:
            ValueError: å¦‚æœç®—æ³•ä¸æ”¯æŒ
        """
        # ç®—æ³•åˆ° Pipeline æ¨¡å—çš„æ˜ å°„
        pipeline_mapping = {
            "baseline": "baseline_rag",
            "longrefiner": "longrefiner_rag",
            "reform": "reform_rag",
            "provence": "provence_rag",
            "longllmlingua": "longllmlingua_rag",
            "llmlingua2": "llmlingua2_rag",
        }

        if algorithm not in pipeline_mapping:
            raise ValueError(
                f"Unknown algorithm: {algorithm}. Supported: {list(pipeline_mapping.keys())}"
            )

        module_name = pipeline_mapping[algorithm]
        self._log(f"   ğŸš€ Starting {module_name} pipeline...")

        # ä½¿ç”¨ importlib åŠ¨æ€å¯¼å…¥ Pipeline æ¨¡å—
        import importlib

        module_path = f"benchmarks.implementations.pipelines.{module_name}"
        pipeline_module = importlib.import_module(module_path)
        pipeline_run_func = pipeline_module.pipeline_run

        # è®¡ç®—é¢„ä¼°ç­‰å¾…æ—¶é—´
        # åŸºäºæ ·æœ¬æ•°å’Œç®—æ³•å¤æ‚åº¦ä¼°ç®—
        base_time_per_sample = {
            "baseline": 3,  # ç§’/æ ·æœ¬
            "longrefiner": 10,
            "reform": 5,
            "provence": 4,
            "longllmlingua": 15,
            "llmlingua2": 4,
        }
        estimated_time = (
            self.config.max_samples * base_time_per_sample.get(algorithm, 5) + 60
        )  # é¢å¤– 60 ç§’ç¼“å†²

        self._log(f"   â±ï¸ Estimated time: {estimated_time}s ({estimated_time // 60}min)")

        # è¿è¡Œ Pipelineï¼ˆåœ¨å•ç‹¬çš„è¿‡ç¨‹ä¸­è¿è¡Œï¼‰
        pipeline_run_func(config)

        # ç­‰å¾… Pipeline å®Œæˆ
        # ä½¿ç”¨ time.sleep() æ˜¯è®¾è®¡è¦æ±‚
        time.sleep(estimated_time)

        self._log(f"   âœ… Pipeline {module_name} completed")


class QualityExperiment(BaseRefinerExperiment):
    """
    è´¨é‡è¯„æµ‹å®éªŒ

    ä¸“æ³¨äºè¯„æµ‹ Refiner å¯¹ç­”æ¡ˆè´¨é‡çš„å½±å“ï¼š
    - F1 Score
    - Recall
    - ROUGE-L
    - Accuracy
    """

    def run(self) -> ExperimentResult:
        """è¿è¡Œè´¨é‡è¯„æµ‹å®éªŒ"""
        # ä½¿ç”¨ ComparisonExperiment çš„é€»è¾‘ï¼Œä½†ä¸“æ³¨äºè´¨é‡æŒ‡æ ‡
        comparison = ComparisonExperiment(self.config)
        return comparison.run()


class LatencyExperiment(BaseRefinerExperiment):
    """
    å»¶è¿Ÿè¯„æµ‹å®éªŒ

    ä¸“æ³¨äºè¯„æµ‹ Refiner çš„å»¶è¿Ÿè¡¨ç°ï¼š
    - Retrieve Time
    - Refine Time
    - Generate Time
    - End-to-End Latency
    """

    def run(self) -> ExperimentResult:
        """è¿è¡Œå»¶è¿Ÿè¯„æµ‹å®éªŒ"""
        # ä½¿ç”¨ ComparisonExperiment çš„é€»è¾‘ï¼Œä½†ä¸“æ³¨äºå»¶è¿ŸæŒ‡æ ‡
        comparison = ComparisonExperiment(self.config)
        return comparison.run()


class CompressionExperiment(BaseRefinerExperiment):
    """
    å‹ç¼©ç‡è¯„æµ‹å®éªŒ

    ä¸“æ³¨äºè¯„æµ‹ Refiner çš„å‹ç¼©æ•ˆæœï¼š
    - Compression Rate
    - Original Tokens
    - Compressed Tokens
    - Token Budget éµå®ˆæƒ…å†µ
    """

    def run(self) -> ExperimentResult:
        """è¿è¡Œå‹ç¼©ç‡è¯„æµ‹å®éªŒ"""
        # ä½¿ç”¨ ComparisonExperiment çš„é€»è¾‘ï¼Œä½†ä¸“æ³¨äºå‹ç¼©æŒ‡æ ‡
        comparison = ComparisonExperiment(self.config)
        return comparison.run()
