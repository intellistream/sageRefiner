#!/usr/bin/env python3
"""
æ‰¾å‡ºæœ€é‡è¦çš„æ£€ç´¢å¤´ - ç®€åŒ–ç‰ˆ

ç”¨æ³•:
    python find_heads.py --model /path/to/model --dataset nq --num_samples 100
"""

import argparse
import logging
import sys
from pathlib import Path

import yaml
from datasets import load_dataset

from benchmarks.analysis.head_analysis import (
    AttentionHookExtractor,
    HeadwiseEvaluator,
)
from benchmarks.analysis.visualization import plot_mnr_curve

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="[%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)


def load_dataset_samples(
    dataset_name: str,
    split: str = "train",
    num_samples: int = 100,
) -> list[dict]:
    """åŠ è½½æ•°æ®é›†æ ·æœ¬

    Args:
        dataset_name: æ•°æ®é›†åç§° (nq, hotpotqa, triviaqa, squad)
        split: æ•°æ®é›†åˆ†å‰²
        num_samples: æ ·æœ¬æ•°é‡

    Returns:
        [{"question": str, "context": str, "answers": list}, ...]
    """
    logger.info(f"Loading dataset: {dataset_name} ({split})")

    # FlashRAG æ•°æ®é›†æ˜ å°„
    dataset_map = {
        "nq": "namespace-Pt/projects",
        "hotpotqa": "hotpot_qa",
        "triviaqa": "trivia_qa",
        "squad": "squad",
    }

    hf_dataset_name = dataset_map.get(dataset_name, dataset_name)

    try:
        ds = load_dataset(hf_dataset_name, split=split, trust_remote_code=True)
    except Exception as e:
        logger.error(f"Failed to load dataset {hf_dataset_name}: {e}")
        logger.info("Falling back to mock data...")
        # ç®€å•çš„ fallback
        return [
            {
                "question": f"What is question {i}?",
                "context": f"This is context for question {i}. " * 50,
                "answers": [f"answer{i}"],
            }
            for i in range(num_samples)
        ]

    samples = []
    ds = ds.select(range(min(num_samples, len(ds))))

    for item in ds:
        # æå– question
        question = item.get("question", item.get("query", ""))

        # æå– context - ä¸æˆªæ–­åˆ°å‰3ä¸ªï¼Œä¿ç•™æ‰€æœ‰æ­£æ ·æœ¬ context
        if "context" in item:
            context = item["context"]
        elif "contexts" in item:
            # ä¿ç•™æ‰€æœ‰ contextsï¼ˆæˆ–è‡³å°‘å‰10ä¸ªä»¥é¿å…è¿‡é•¿ï¼‰
            contexts = item["contexts"]
            if isinstance(contexts, list):
                context = " ".join(contexts[:10])  # æœ€å¤šå–å‰10ä¸ª
            else:
                context = str(contexts)
        elif "positive_ctxs" in item:
            # ä¿ç•™æ‰€æœ‰ positive contextsï¼ˆæˆ–è‡³å°‘å‰10ä¸ªï¼‰
            positive_ctxs = item["positive_ctxs"]
            if isinstance(positive_ctxs, list):
                context = " ".join([ctx.get("text", "") for ctx in positive_ctxs[:10]])
            else:
                context = str(positive_ctxs)
        else:
            # å¦‚æœæ²¡æœ‰ contextï¼Œä½¿ç”¨é—®é¢˜æœ¬èº«ï¼ˆä¸ç†æƒ³ä½†å¯ä»¥å·¥ä½œï¼‰
            context = f"Context for: {question}"

        # æå– answers
        answers = item.get("answers", item.get("answer", []))
        if isinstance(answers, str):
            answers = [answers]

        if question and context:
            samples.append(
                {
                    "question": question,
                    "context": context,
                    "answers": answers,
                }
            )

        if len(samples) >= num_samples:
            break

    logger.info(f"Loaded {len(samples)} samples")
    return samples


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Find important retrieval heads")

    # åªæ¥å—é…ç½®æ–‡ä»¶
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to YAML config file",
    )

    args = parser.parse_args()

    # ä»é…ç½®æ–‡ä»¶åŠ è½½æ‰€æœ‰é…ç½®
    logger.info(f"Loading config from: {args.config}")
    with open(args.config) as f:
        config = yaml.safe_load(f)

    # åˆ›å»ºé…ç½®å¯¹è±¡
    class Config:
        pass

    cfg = Config()
    for key, value in config.items():
        setattr(cfg, key, value)

    logger.info("=" * 80)
    logger.info("Attention Head Selection for Retrieval")
    logger.info("=" * 80)
    logger.info(f"Model: {cfg.model}")
    logger.info(f"Dataset: {cfg.dataset}")
    logger.info(f"Samples: {cfg.num_samples}")
    logger.info(f"Device: {cfg.device}")
    logger.info("=" * 80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(cfg.output_dir) / f"{cfg.dataset}_analysis"
    output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"Output directory: {output_dir}")

    # åŠ è½½æ•°æ®é›†
    samples = load_dataset_samples(cfg.dataset, cfg.get("split", "train"), cfg.num_samples)

    if not samples:
        logger.error("No samples loaded!")
        return

    # åˆ›å»ºæ¨¡å‹æå–å™¨
    logger.info("\n" + "-" * 80)
    logger.info("Loading Model and Registering Hooks")
    logger.info("-" * 80)

    layer_range = None
    if hasattr(cfg, "layer_end") and cfg.layer_end is not None:
        layer_start = getattr(cfg, "layer_start", 0)
        layer_range = (layer_start, cfg.layer_end)

    extractor = AttentionHookExtractor(
        model_name=cfg.model,
        dtype=getattr(cfg, "dtype", "bfloat16"),
        device=cfg.device,
        layer_range=layer_range,
    )

    extractor.register_hooks()

    # åˆ›å»ºè¯„ä¼°å™¨
    logger.info("\n" + "-" * 80)
    logger.info("Initializing Evaluator")
    logger.info("-" * 80)

    evaluator = HeadwiseEvaluator(
        model_extractor=extractor,
        query_pooling="max",
        context_pooling="mean",
        output_top_k=getattr(cfg, "top_k", 20),
    )

    # è¿è¡Œè¯„ä¼°
    logger.info("\n" + "-" * 80)
    logger.info("Running Evaluation")
    logger.info("-" * 80)

    results_df = evaluator.evaluate_samples(samples, log_interval=10)

    # ä¿å­˜ç»“æœ
    logger.info("\n" + "-" * 80)
    logger.info("Saving Results")
    logger.info("-" * 80)

    evaluator.save_results(results_df, output_dir, cfg.dataset)

    # æ˜¾ç¤º top heads
    top_heads = evaluator.get_top_heads(results_df)

    logger.info("\n" + "=" * 80)
    logger.info(f"ğŸ” Top-{getattr(cfg, 'top_k', 20)} Heads for {cfg.dataset.upper()}")
    logger.info("=" * 80)

    for idx, row in top_heads.iterrows():
        logger.info(
            f"  {idx + 1:2d}. {row['head_type']}-Head "
            f"Layer {int(row['layer']):2d} Head {int(row['head']):2d}: "
            f"MNR = {row['mnr']:.4f} (Â±{row['mnr_std']:.4f})"
        )

    # ç”Ÿæˆå¯è§†åŒ–
    logger.info("\n" + "-" * 80)
    logger.info("Generating Visualizations")
    logger.info("-" * 80)

    viz_dir = output_dir / "visualizations"
    viz_dir.mkdir(exist_ok=True)

    try:
        plot_mnr_curve(
            results_df,
            viz_dir / "mnr_curve.png",
            dataset_name=cfg.dataset,
        )
        logger.info(f"Saved visualization to {viz_dir}")
    except Exception as e:
        logger.warning(f"Failed to generate visualization: {e}")

    # ä¿å­˜é…ç½®
    config_path = output_dir / "config.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f, default_flow_style=False)
    logger.info(f"Saved config to {config_path}")

    # æ¸…ç†
    extractor.remove_hooks()

    logger.info("\n" + "=" * 80)
    logger.info("âœ… Analysis Complete!")
    logger.info("=" * 80)
    logger.info(f"Results: {output_dir}")
    best = top_heads.iloc[0]
    logger.info(
        f"Best head: {best['head_type']}-Head "
        f"Layer {int(best['layer'])} Head {int(best['head'])} "
        f"(MNR={best['mnr']:.4f})"
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\n\nInterrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\n\nFailed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
