pipeline:
  name: "sage-benchmark-longllmlingua-rag"
  description: "LongLLMLingua RAG Pipeline for Long Document Compression"
  version: "1.0.0"

source:
  # 数据源类型：'local'（本地 JSONL） 或 'hf'（HuggingFace Dataset）
  type: "hf"
  # HuggingFace Dataset 参数
  hf_dataset_name: "RUC-NLPIR/FlashRAG_datasets"
  hf_dataset_config: "nq"  # Natural Questions dataset
  hf_split: "test"
  max_samples: 20

retriever:
  # 检索器类型: wiki18_faiss
  type: "wiki18_faiss"

  # 通用配置
  dimension: 1024    # BGE-Large-EN-v1.5模型的维度
  top_k: 100

  # Wiki18 FAISS 专用配置
  faiss:
    index_path: "/home/cyb/wiki18_maxp.index"
    documents_path: "/home/cyb/wiki18_fulldoc.jsonl"
    mapping_path: "/home/cyb/wiki18_maxp_maxp_mapping.json"  # 段落到文档的映射

  # 嵌入模型配置
  embedding:
    method: "hf"
    model: "BAAI/bge-large-en-v1.5"  # BGE-Large-EN-v1.5模型
    gpu_device: 0

generator:
  vllm:
    api_key: "token-abc123"
    method: "openai"
    model_name: "/home/cyb/Llama-3.1-8B-Instruct"
    base_url: "http://sage2:8000/v1"
    seed: 42

promptor:
  platform: "local"

longllmlingua:
  # LongLLMLingua 压缩配置 (Paper Baseline)
  # 参考论文: https://arxiv.org/abs/2310.06839
  enabled: true  # 设为 false 即为 baseline 模式

  # 模型配置
  # LongLLMLingua 使用 LLM 计算 perplexity 进行压缩
  model_name: "NousResearch/Llama-2-7b-hf"  # 或 meta-llama/Llama-2-7b-hf
  device: "cuda:0"

  # 压缩参数 (Paper Baseline Settings)
  rate: 0.55                               # 压缩率 (0-1)，论文默认 0.55
  target_token: -1                         # 目标 token 数，-1 表示使用 rate

  # LongLLMLingua 特有参数
  condition_in_question: "after"           # Question 放在 context 之后计算 PPL
  reorder_context: "sort"                  # 按相关性排序上下文
  dynamic_context_compression_ratio: 0.3   # 动态压缩比例
  condition_compare: true                  # 启用对比 perplexity (paper baseline)

  # 过滤选项
  use_context_level_filter: true           # 启用文档级过滤
  use_sentence_level_filter: false         # 句子级过滤 (默认关闭)
  use_token_level_filter: true             # 启用 token 级压缩

  # Token 预算
  context_budget: "+100"                   # 上下文 token 预算表达式

  # 迭代压缩
  iterative_size: 200                      # 每次迭代处理的 token 数

sink:
  platform: "local"

evaluate:
  platform: "local"
