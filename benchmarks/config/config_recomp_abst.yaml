pipeline:
  name: "sage-benchmark-recomp-abst-rag"
  description: "RECOMP Abstractive RAG Pipeline with T5-Based Summarization Compression"
  version: "1.0.0"

source:
  # 数据源类型：'local'（本地 JSONL） 或 'hf'（HuggingFace Dataset）
  type: "hf"
  # HuggingFace Dataset 参数
  hf_dataset_name: "RUC-NLPIR/FlashRAG_datasets"
  hf_dataset_config: "nq"  # Natural Questions dataset
  hf_split: "test"
  max_samples: 20

retriever:
  # 检索器类型: wiki18_faiss
  type: "wiki18_faiss"

  # 通用配置
  dimension: 1024    # BGE-Large-EN-v1.5模型的维度
  top_k: 100

  # Wiki18 FAISS 专用配置
  faiss:
    index_path: "/home/cyb/wiki18_maxp.index"
    documents_path: "/home/cyb/wiki18_fulldoc.jsonl"
    mapping_path: "/home/cyb/wiki18_maxp_maxp_mapping.json"  # 段落到文档的映射

  # 嵌入模型配置
  embedding:
    method: "hf"
    model: "BAAI/bge-large-en-v1.5"  # BGE-Large-EN-v1.5模型
    gpu_device: 0

generator:
  vllm:
    api_key: "token-abc123"
    method: "openai"
    model_name: "/home/cyb/Llama-3.1-8B-Instruct"
    base_url: "http://sage2:8000/v1"
    seed: 42

promptor:
  platform: "local"

recomp_abst:
  # RECOMP Abstractive压缩配置
  enabled: true  # 设为false即为baseline模式

  # 模型配置
  # 支持的模型:
  # - fangyuan/nq_abstractive_compressor (NQ 数据集微调，推荐用于 NQ)
  # - fangyuan/tqa_abstractive_compressor (TriviaQA 微调)
  # - fangyuan/hotpotqa_abstractive (HotpotQA 微调)
  # - google/flan-t5-base, google/flan-t5-large (通用摘要模型)
  model_path: "fangyuan/nq_abstractive_compressor"
  device: "cuda"               # 运行设备 ("cuda", "cpu", 或 null 自动检测)

  # 生成参数
  max_source_length: 1024      # 输入最大长度（包含question和documents）
  max_target_length: 512       # 生成摘要的最大长度
  num_beams: 4                 # beam search的beam数量
  length_penalty: 1.0          # 长度惩罚系数（>1.0鼓励更长输出，<1.0鼓励更短输出）
  early_stopping: true         # 是否在所有beam完成后提前停止
  no_repeat_ngram_size: 3      # 禁止重复的n-gram大小

  # 可选：模型缓存目录
  # cache_dir: "/path/to/model/cache"

sink:
  platform: "local"

evaluate:
  platform: "local"
